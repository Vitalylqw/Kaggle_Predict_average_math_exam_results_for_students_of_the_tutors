{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = 'train.csv'\n",
    "TEST = 'test.csv'\n",
    "df_correct_file = 'df_correct.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN)\n",
    "df_correct = pd.read_csv(df_correct_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('x_train.csv')\n",
    "X_train=X.to_numpy()\n",
    "X_test = pd.read_csv('x_test.csv')\n",
    "y = pd.read_csv('y_train.csv')\n",
    "y_train = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTreeFastMse():\n",
    "\n",
    "    \n",
    "    # объявляем характеристики класса\n",
    "    def __init__(self, max_depth=8, min_size=5):\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.min_size = min_size\n",
    "        self.value = 0\n",
    "        self.feature_idx = -1\n",
    "        self.feature_threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        \n",
    "    # процедура обучения - сюда передается обучающая выборка\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # начальное значение - среднее значение y\n",
    "        self.value = y.mean()\n",
    "        # начальная ошибка - mse между значением в листе (пока нет\n",
    "        # разбиения, это среднее по всем объектам) и объектами\n",
    "        base_error = ((y - self.value) ** 2).sum()\n",
    "        error = base_error\n",
    "        flag = 0\n",
    "        \n",
    "        # пришли в максимальную глубину\n",
    "        if self.max_depth <= 1:\n",
    "            return\n",
    "    \n",
    "        dim_shape = X.shape[1]\n",
    "        \n",
    "        left_value, right_value = 0, 0\n",
    "        \n",
    "        for feat in range(dim_shape):\n",
    "            \n",
    "            prev_error1, prev_error2 = base_error, 0 \n",
    "            idxs = np.argsort(X[:, feat])\n",
    "            \n",
    "            # переменные для быстрого переброса суммы\n",
    "            mean1, mean2 = y.mean(), 0\n",
    "            sm1, sm2 = y.sum(), 0\n",
    "            \n",
    "            N = X.shape[0]\n",
    "            N1, N2 = N, 0\n",
    "            thres = 1\n",
    "            \n",
    "            while thres < N - 1:\n",
    "                N1 -= 1\n",
    "                N2 += 1\n",
    "\n",
    "                idx = idxs[thres]\n",
    "                x = X[idx, feat]\n",
    "                \n",
    "                # вычисляем дельты - по ним в основном будет делаться переброс\n",
    "                delta1 = (sm1 - y[idx]) * 1.0 / N1 - mean1\n",
    "                delta2 = (sm2 + y[idx]) * 1.0 / N2 - mean2\n",
    "                \n",
    "                # увеличиваем суммы\n",
    "                sm1 -= y[idx]\n",
    "                sm2 += y[idx]\n",
    "                \n",
    "                # пересчитываем ошибки за O(1)\n",
    "                prev_error1 += (delta1**2) * N1 \n",
    "                prev_error1 -= (y[idx] - mean1)**2 \n",
    "                prev_error1 -= 2 * delta1 * (sm1 - mean1 * N1)\n",
    "                mean1 = sm1/N1\n",
    "                \n",
    "                prev_error2 += (delta2**2) * N2 \n",
    "                prev_error2 += (y[idx] - mean2)**2 \n",
    "                prev_error2 -= 2 * delta2 * (sm2 - mean2 * N2)\n",
    "                mean2 = sm2/N2\n",
    "                \n",
    "                # пропускаем близкие друг к другу значения\n",
    "                if thres < N - 1 and np.abs(x - X[idxs[thres + 1], feat]) < 1e-5:\n",
    "                    thres += 1\n",
    "                    continue\n",
    "                \n",
    "                # 2 условия, чтобы осуществить сплит - уменьшение ошибки \n",
    "                # и минимальное кол-о эл-в в каждом листе\n",
    "                if (prev_error1 + prev_error2 < error):\n",
    "                    if (min(N1,N2) > self.min_size):\n",
    "                    \n",
    "                        # переопределяем самый лучший признак и границу по нему\n",
    "                        self.feature_idx, self.feature_threshold = feat, x\n",
    "                        # переопределяем значения в листах\n",
    "                        left_value, right_value = mean1, mean2\n",
    "\n",
    "                        # флаг - значит сделали хороший сплит\n",
    "                        flag = 1\n",
    "                        error = prev_error1 + prev_error2\n",
    "                                     \n",
    "                thres += 1\n",
    " \n",
    "        # ничего не разделили, выходим\n",
    "        if self.feature_idx == -1:\n",
    "            return\n",
    "        \n",
    "        self.left = RegressionTreeFastMse(self.max_depth - 1)\n",
    "        # print (\"Левое поддерево с глубиной %d\"%(self.max_depth - 1))\n",
    "        self.left.value = left_value\n",
    "        self.right = RegressionTreeFastMse(self.max_depth - 1)\n",
    "        # print (\"Правое поддерево с глубиной %d\"%(self.max_depth - 1))\n",
    "        self.right.value = right_value\n",
    "        \n",
    "        idxs_l = (X[:, self.feature_idx] > self.feature_threshold)\n",
    "        idxs_r = (X[:, self.feature_idx] <= self.feature_threshold)\n",
    "    \n",
    "        self.left.fit(X[idxs_l, :], y[idxs_l])\n",
    "        self.right.fit(X[idxs_r, :], y[idxs_r])\n",
    "        \n",
    "    def __predict(self, x):\n",
    "        if self.feature_idx == -1:\n",
    "            return self.value\n",
    "        \n",
    "        if x[self.feature_idx] > self.feature_threshold:\n",
    "            return self.left.__predict(x)\n",
    "        else:\n",
    "            return self.right.__predict(x)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y = np.zeros(X.shape[0])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            y[i] = self.__predict(X[i])    \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class g_boost():   \n",
    "    def __init__(self,n_trees=10, max_depth=3, eta=1,min_size=5):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.eta = eta\n",
    "        self.min_size = min_size\n",
    "        self.trees = []      \n",
    "   \n",
    "    def bias(self,y, z):\n",
    "        return (y - z)\n",
    "    \n",
    "    def mean_squared_error(self,y_real, prediction):\n",
    "        return (sum((y_real - prediction)**2)) / len(y_real)\n",
    "    \n",
    "    def R2(self,y,y_pred):\n",
    "        return 1- ((y_pred - y)**2).sum()/((y-y.mean())**2).sum()\n",
    "      \n",
    "    def predict(self,X):\n",
    "        # Реализуемый алгоритм градиентного бустинга будет инициализироваться нулевыми значениями,\n",
    "        # поэтому все деревья из списка trees_list уже являются дополнительными и при предсказании прибавляются с шагом eta\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for alg in self.trees:\n",
    "            y_pred+=self.eta * alg.predict(X)     \n",
    "        return y_pred\n",
    "     \n",
    "    def fit(self,X_train,y_train): \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    # Деревья будем записывать в список\n",
    "        for i in range(self.n_trees):\n",
    "            tree = RegressionTreeFastMse(max_depth=self.max_depth,min_size = self.min_size)\n",
    "            # инициализируем бустинг начальным алгоритмом, возвращающим ноль, \n",
    "            # поэтому первый алгоритм просто обучаем на выборке и добавляем в список\n",
    "            if len(self.trees) == 0:\n",
    "                # обучаем первое дерево на обучающей выборке\n",
    "                tree.fit(X_train, y_train)\n",
    "            else:\n",
    "                # Получим ответы на текущей композиции\n",
    "                target = self.predict(X_train)          \n",
    "                # алгоритмы начиная со второго обучаем на сдвиг\n",
    "                tree.fit(X_train, self.bias(y_train, target))        \n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def  train_test(self,metrika = 'r2'):\n",
    "        if metrika!='r2':\n",
    "            return self.mean_squared_error(self.y_train,self.predict(self.X_train))\n",
    "        else:\n",
    "            return self.R2(self.y_train,self.predict(self.X_train))\n",
    "    \n",
    "    def metrika_test(self, X_test,y_test, metrika = 'r2'):\n",
    "        if metrika!='r2':\n",
    "            return self.mean_squared_error(y_test,self.predict(X_test))\n",
    "        else:\n",
    "            return self.R2(y_test,self.predict(X_test))\n",
    "        \n",
    "def cross_validation(n,size):\n",
    "        data=[]\n",
    "        r = np.arange(n)\n",
    "        for i in range(size-1):\n",
    "            ind = np.random.choice(r,size=int(n/size), replace=False)\n",
    "            data.append(ind)\n",
    "            r=np.setdiff1d(r,ind) \n",
    "        data.append(r)\n",
    "        return data\n",
    "    \n",
    "def test_cros(X,y,size=5,print_res =True,n_trees=10,max_depth=8,eta=0.5,min_size=5):\n",
    "        cros_val = cross_validation(X.shape[0],size)\n",
    "        data=[]\n",
    "        for i in range(len(cros_val)):\n",
    "            ind_train = np.hstack((cros_val[:i]+cros_val[i+1:]))\n",
    "            ind_test = cros_val[i]\n",
    "            forest = g_boost(n_trees=n_trees, max_depth=max_depth, eta=eta,min_size=min_size)\n",
    "            forest.fit(X[ind_train],y[ind_train])\n",
    "            r2_test = forest.metrika_test(X[ind_test],y[ind_test])             \n",
    "            r2_train = forest.train_test()\n",
    "            data.append((r2_test,r2_train))\n",
    "            if print_res:\n",
    "                print(f'Выборка номер {i} r2 на обучении {r2_train}, r2 на тесте {r2_test}')              \n",
    "        return data             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате огромного тмножества эксперементов над параметрами, гиперпаораметрами и\n",
    "набором данных были подобраны лучшие\n",
    "n_trees=50,max_depth=5,eta=0.2,min_size=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выборка номер 0 r2 на обучении 0.7934621941146989, r2 на тесте 0.7859003533647789\n",
      "Выборка номер 1 r2 на обучении 0.7917654745451445, r2 на тесте 0.7838205327776654\n",
      "Выборка номер 2 r2 на обучении 0.794826725983474, r2 на тесте 0.7755099495962103\n",
      "Выборка номер 3 r2 на обучении 0.7941052845207411, r2 на тесте 0.7758350297608734\n",
      "Выборка номер 4 r2 на обучении 0.7958767574971524, r2 на тесте 0.776769654211275\n"
     ]
    }
   ],
   "source": [
    "test =test_cros(X_train,np.ravel(y_train),n_trees=50,max_depth=5,eta=0.2, min_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
